{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYoZZsqiFiIvdBf/BSxeiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eileenjang/deeplearning_practice/blob/main/pytorch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U-E82nlMdI8"
      },
      "source": [
        "#git branch test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA6GF2wgr-Cb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PdDlnFVluR4B",
        "outputId": "d3c07c6e-0637-4afb-91e1-e42dd67a3f99"
      },
      "source": [
        "no = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "area = [3400,4500,6500,7700,6600,3500,4300,3700,8700,9000,9100,9200,8200,8800,8400,8300,7900,7400,7100,6400]\n",
        "price = [1500,2300,2777,3567,2799,1633,2111,1700,4333,4500,4551,4560,4200,4304,4000,4230,3700,3300,3000,2600]\n",
        "data = pd.DataFrame({\"No\":no,\"Area\":area,\"Price\":price})\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Area</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3400</td>\n",
              "      <td>1500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4500</td>\n",
              "      <td>2300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>6500</td>\n",
              "      <td>2777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>7700</td>\n",
              "      <td>3567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6600</td>\n",
              "      <td>2799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  Area  Price\n",
              "0   1  3400   1500\n",
              "1   2  4500   2300\n",
              "2   3  6500   2777\n",
              "3   4  7700   3567\n",
              "4   5  6600   2799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "u9ntclOtuYle",
        "outputId": "ad5e00a5-cae9-41f4-ddd3-31d39dfc8d93"
      },
      "source": [
        "index = [1,2,3]\n",
        "area_size = [1200, 1350, 5400]\n",
        "area_value = [571, 643, 2570]\n",
        "test_data = pd.DataFrame({\"No\":index, \"Area\": area_size, \"Expected Value\": area_value})\n",
        " \n",
        "test_data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Area</th>\n",
              "      <th>Expected Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1200</td>\n",
              "      <td>571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1350</td>\n",
              "      <td>643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5400</td>\n",
              "      <td>2570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  Area  Expected Value\n",
              "0   1  1200             571\n",
              "1   2  1350             643\n",
              "2   3  5400            2570"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8iYJflqueMv"
      },
      "source": [
        "data = np.array(data,dtype=np.float32)\n",
        "x_train = data[: ,1:2]\n",
        "y_train = data[: ,2:3]\n",
        " \n",
        "test_data = np.array(test_data,dtype=np.float32)\n",
        "x_test = test_data[:,1:2]\n",
        "y_test = test_data[:, 2:3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "JEqNcQpbuwe6",
        "outputId": "2f025e7e-be0d-4d33-d4c7-54cff86578dd"
      },
      "source": [
        "plt.plot(x_train, y_train, \"bo\")\n",
        "plt.xlabel(\"Price ($) of land\")\n",
        "plt.ylabel(\"Size of Land\")\n",
        "plt.title(\"Land purchase trends\")\n",
        "plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T7VkWw0hx7_c",
        "outputId": "d46e9e15-9c29-4ada-d540-1c8a3e0ac256"
      },
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0  # a random guess: random value, 1.0\n",
        "\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "   return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "   y_pred = forward(x)\n",
        "   return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "w_list = []\n",
        "mse_list = []\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "   print(\"w=\", w)\n",
        "   l_sum = 0\n",
        "   for x_val, y_val in zip(x_data, y_data):\n",
        "       y_pred_val = forward(x_val)\n",
        "       l = loss(x_val, y_val)\n",
        "       l_sum += l\n",
        "       print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "   print(\"MSE=\", l_sum / 3)\n",
        "   w_list.append(w)\n",
        "   mse_list.append(l_sum / 3)\n",
        "\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9D16IOu0TD2"
      },
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0  # a random guess: random value\n",
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "   return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "   y_pred = forward(x)\n",
        "   return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "   return 2 * x * (x * w - y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQRFRvMl0Wtf",
        "outputId": "089cf8a4-f769-4006-c572-ca08091e8b85"
      },
      "source": [
        "print(\"predict (before training)\",  4, forward(4))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "   for x_val, y_val in zip(x_data, y_data):\n",
        "       grad = gradient(x_val, y_val)\n",
        "       w = w - 0.01 * grad\n",
        "       print(\"\\tgrad: \", x_val, y_val, grad)\n",
        "       l = loss(x_val, y_val)\n",
        "\n",
        "   print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\",  \"4 hours\", forward(4))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.2288\n",
            "progress: 0 w= 1.260688 loss= 4.919240100095999\n",
            "\tgrad:  1.0 2.0 -1.478624\n",
            "\tgrad:  2.0 4.0 -5.796206079999999\n",
            "\tgrad:  3.0 6.0 -11.998146585599997\n",
            "progress: 1 w= 1.453417766656 loss= 2.688769240265834\n",
            "\tgrad:  1.0 2.0 -1.093164466688\n",
            "\tgrad:  2.0 4.0 -4.285204709416961\n",
            "\tgrad:  3.0 6.0 -8.87037374849311\n",
            "progress: 2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
            "\tgrad:  1.0 2.0 -0.8081896081960389\n",
            "\tgrad:  2.0 4.0 -3.1681032641284723\n",
            "\tgrad:  3.0 6.0 -6.557973756745939\n",
            "progress: 3 w= 1.701247862192685 loss= 0.8032755585999681\n",
            "\tgrad:  1.0 2.0 -0.59750427561463\n",
            "\tgrad:  2.0 4.0 -2.3422167604093502\n",
            "\tgrad:  3.0 6.0 -4.848388694047353\n",
            "progress: 4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
            "\tgrad:  1.0 2.0 -0.44174208101320334\n",
            "\tgrad:  2.0 4.0 -1.7316289575717576\n",
            "\tgrad:  3.0 6.0 -3.584471942173538\n",
            "progress: 5 w= 1.836707389300983 loss= 0.2399802903801062\n",
            "\tgrad:  1.0 2.0 -0.3265852213980338\n",
            "\tgrad:  2.0 4.0 -1.2802140678802925\n",
            "\tgrad:  3.0 6.0 -2.650043120512205\n",
            "progress: 6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
            "\tgrad:  1.0 2.0 -0.241448373202223\n",
            "\tgrad:  2.0 4.0 -0.946477622952715\n",
            "\tgrad:  3.0 6.0 -1.9592086795121197\n",
            "progress: 7 w= 1.910747160155559 loss= 0.07169462478267678\n",
            "\tgrad:  1.0 2.0 -0.17850567968888198\n",
            "\tgrad:  2.0 4.0 -0.6997422643804168\n",
            "\tgrad:  3.0 6.0 -1.4484664872674653\n",
            "progress: 8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
            "\tgrad:  1.0 2.0 -0.13197139106214673\n",
            "\tgrad:  2.0 4.0 -0.5173278529636143\n",
            "\tgrad:  3.0 6.0 -1.0708686556346834\n",
            "progress: 9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
            "\tgrad:  1.0 2.0 -0.09756803306893769\n",
            "\tgrad:  2.0 4.0 -0.38246668963023644\n",
            "\tgrad:  3.0 6.0 -0.7917060475345892\n",
            "progress: 10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
            "\tgrad:  1.0 2.0 -0.07213321766426262\n",
            "\tgrad:  2.0 4.0 -0.2827622132439096\n",
            "\tgrad:  3.0 6.0 -0.5853177814148953\n",
            "progress: 11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
            "\tgrad:  1.0 2.0 -0.05332895341780164\n",
            "\tgrad:  2.0 4.0 -0.2090494973977819\n",
            "\tgrad:  3.0 6.0 -0.4327324596134101\n",
            "progress: 12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
            "\tgrad:  1.0 2.0 -0.039426735209221686\n",
            "\tgrad:  2.0 4.0 -0.15455280202014876\n",
            "\tgrad:  3.0 6.0 -0.3199243001817109\n",
            "progress: 13 w= 1.9854256707695 loss= 0.001911699652671057\n",
            "\tgrad:  1.0 2.0 -0.02914865846100012\n",
            "\tgrad:  2.0 4.0 -0.11426274116712065\n",
            "\tgrad:  3.0 6.0 -0.2365238742159388\n",
            "progress: 14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
            "\tgrad:  1.0 2.0 -0.021549952984118992\n",
            "\tgrad:  2.0 4.0 -0.08447581569774698\n",
            "\tgrad:  3.0 6.0 -0.17486493849433593\n",
            "progress: 15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
            "\tgrad:  1.0 2.0 -0.015932138840594856\n",
            "\tgrad:  2.0 4.0 -0.062453984255132156\n",
            "\tgrad:  3.0 6.0 -0.12927974740812687\n",
            "progress: 16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
            "\tgrad:  1.0 2.0 -0.011778821430517894\n",
            "\tgrad:  2.0 4.0 -0.046172980007630926\n",
            "\tgrad:  3.0 6.0 -0.09557806861579543\n",
            "progress: 17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
            "\tgrad:  1.0 2.0 -0.008708224029438938\n",
            "\tgrad:  2.0 4.0 -0.03413623819540135\n",
            "\tgrad:  3.0 6.0 -0.07066201306448505\n",
            "progress: 18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
            "\tgrad:  1.0 2.0 -0.006438094523652627\n",
            "\tgrad:  2.0 4.0 -0.02523733053271826\n",
            "\tgrad:  3.0 6.0 -0.052241274202728505\n",
            "progress: 19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
            "\tgrad:  1.0 2.0 -0.004759760538470381\n",
            "\tgrad:  2.0 4.0 -0.01865826131080439\n",
            "\tgrad:  3.0 6.0 -0.03862260091336722\n",
            "progress: 20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
            "\tgrad:  1.0 2.0 -0.0035189480832178432\n",
            "\tgrad:  2.0 4.0 -0.01379427648621423\n",
            "\tgrad:  3.0 6.0 -0.028554152326460525\n",
            "progress: 21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
            "\tgrad:  1.0 2.0 -0.002601600545300009\n",
            "\tgrad:  2.0 4.0 -0.01019827413757568\n",
            "\tgrad:  3.0 6.0 -0.021110427464781978\n",
            "progress: 22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
            "\tgrad:  1.0 2.0 -0.001923394502346909\n",
            "\tgrad:  2.0 4.0 -0.007539706449199102\n",
            "\tgrad:  3.0 6.0 -0.01560719234984198\n",
            "progress: 23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
            "\tgrad:  1.0 2.0 -0.0014219886363191492\n",
            "\tgrad:  2.0 4.0 -0.005574195454370212\n",
            "\tgrad:  3.0 6.0 -0.011538584590544687\n",
            "progress: 24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
            "\tgrad:  1.0 2.0 -0.0010512932626940419\n",
            "\tgrad:  2.0 4.0 -0.004121069589761106\n",
            "\tgrad:  3.0 6.0 -0.008530614050808794\n",
            "progress: 25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
            "\tgrad:  1.0 2.0 -0.0007772337246287897\n",
            "\tgrad:  2.0 4.0 -0.0030467562005451754\n",
            "\tgrad:  3.0 6.0 -0.006306785335127074\n",
            "progress: 26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
            "\tgrad:  1.0 2.0 -0.0005746182194226179\n",
            "\tgrad:  2.0 4.0 -0.002252503420136165\n",
            "\tgrad:  3.0 6.0 -0.00466268207967957\n",
            "progress: 27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
            "\tgrad:  1.0 2.0 -0.0004248221450375844\n",
            "\tgrad:  2.0 4.0 -0.0016653028085471533\n",
            "\tgrad:  3.0 6.0 -0.0034471768136938863\n",
            "progress: 28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
            "\tgrad:  1.0 2.0 -0.00031407610969225175\n",
            "\tgrad:  2.0 4.0 -0.0012311783499932005\n",
            "\tgrad:  3.0 6.0 -0.0025485391844828342\n",
            "progress: 29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
            "\tgrad:  1.0 2.0 -0.00023220023680847746\n",
            "\tgrad:  2.0 4.0 -0.0009102249282886277\n",
            "\tgrad:  3.0 6.0 -0.0018841656015560204\n",
            "progress: 30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
            "\tgrad:  1.0 2.0 -0.00017166842147497974\n",
            "\tgrad:  2.0 4.0 -0.0006729402121816719\n",
            "\tgrad:  3.0 6.0 -0.0013929862392156878\n",
            "progress: 31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
            "\tgrad:  1.0 2.0 -0.0001269165240174175\n",
            "\tgrad:  2.0 4.0 -0.0004975127741477792\n",
            "\tgrad:  3.0 6.0 -0.0010298514424817995\n",
            "progress: 32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
            "\tgrad:  1.0 2.0 -9.383090920422887e-05\n",
            "\tgrad:  2.0 4.0 -0.00036781716408107457\n",
            "\tgrad:  3.0 6.0 -0.0007613815296476645\n",
            "progress: 33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
            "\tgrad:  1.0 2.0 -6.937031714571162e-05\n",
            "\tgrad:  2.0 4.0 -0.0002719316432120422\n",
            "\tgrad:  3.0 6.0 -0.0005628985014531906\n",
            "progress: 34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
            "\tgrad:  1.0 2.0 -5.1286307909848006e-05\n",
            "\tgrad:  2.0 4.0 -0.00020104232700646207\n",
            "\tgrad:  3.0 6.0 -0.0004161576169003922\n",
            "progress: 35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
            "\tgrad:  1.0 2.0 -3.7916582873442906e-05\n",
            "\tgrad:  2.0 4.0 -0.0001486330048638962\n",
            "\tgrad:  3.0 6.0 -0.0003076703200690645\n",
            "progress: 36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
            "\tgrad:  1.0 2.0 -2.8032184717474706e-05\n",
            "\tgrad:  2.0 4.0 -0.0001098861640933535\n",
            "\tgrad:  3.0 6.0 -0.00022746435967313516\n",
            "progress: 37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
            "\tgrad:  1.0 2.0 -2.0724530547688857e-05\n",
            "\tgrad:  2.0 4.0 -8.124015974608767e-05\n",
            "\tgrad:  3.0 6.0 -0.00016816713067413502\n",
            "progress: 38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
            "\tgrad:  1.0 2.0 -1.5321894128117464e-05\n",
            "\tgrad:  2.0 4.0 -6.006182498197177e-05\n",
            "\tgrad:  3.0 6.0 -0.00012432797771566584\n",
            "progress: 39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
            "\tgrad:  1.0 2.0 -1.1327660191629008e-05\n",
            "\tgrad:  2.0 4.0 -4.4404427951505454e-05\n",
            "\tgrad:  3.0 6.0 -9.191716585732479e-05\n",
            "progress: 40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
            "\tgrad:  1.0 2.0 -8.37467511161094e-06\n",
            "\tgrad:  2.0 4.0 -3.282872643772805e-05\n",
            "\tgrad:  3.0 6.0 -6.795546372551087e-05\n",
            "progress: 41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
            "\tgrad:  1.0 2.0 -6.191497806007362e-06\n",
            "\tgrad:  2.0 4.0 -2.4270671399762023e-05\n",
            "\tgrad:  3.0 6.0 -5.0240289795056015e-05\n",
            "progress: 42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
            "\tgrad:  1.0 2.0 -4.5774486259198e-06\n",
            "\tgrad:  2.0 4.0 -1.794359861406747e-05\n",
            "\tgrad:  3.0 6.0 -3.714324913239864e-05\n",
            "progress: 43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
            "\tgrad:  1.0 2.0 -3.3841626985164908e-06\n",
            "\tgrad:  2.0 4.0 -1.326591777761621e-05\n",
            "\tgrad:  3.0 6.0 -2.7460449796734565e-05\n",
            "progress: 44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
            "\tgrad:  1.0 2.0 -2.5019520926150562e-06\n",
            "\tgrad:  2.0 4.0 -9.807652203264183e-06\n",
            "\tgrad:  3.0 6.0 -2.0301840059744336e-05\n",
            "progress: 45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
            "\tgrad:  1.0 2.0 -1.8497232057157476e-06\n",
            "\tgrad:  2.0 4.0 -7.250914967116273e-06\n",
            "\tgrad:  3.0 6.0 -1.5009393983689279e-05\n",
            "progress: 46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
            "\tgrad:  1.0 2.0 -1.3675225627451937e-06\n",
            "\tgrad:  2.0 4.0 -5.3606884460322135e-06\n",
            "\tgrad:  3.0 6.0 -1.109662508014253e-05\n",
            "progress: 47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
            "\tgrad:  1.0 2.0 -1.0110258408246864e-06\n",
            "\tgrad:  2.0 4.0 -3.963221296032771e-06\n",
            "\tgrad:  3.0 6.0 -8.20386808086937e-06\n",
            "progress: 48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
            "\tgrad:  1.0 2.0 -7.474635363990956e-07\n",
            "\tgrad:  2.0 4.0 -2.930057062755509e-06\n",
            "\tgrad:  3.0 6.0 -6.065218119744031e-06\n",
            "progress: 49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
            "\tgrad:  1.0 2.0 -5.526087618612507e-07\n",
            "\tgrad:  2.0 4.0 -2.166226346744793e-06\n",
            "\tgrad:  3.0 6.0 -4.484088535150477e-06\n",
            "progress: 50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
            "\tgrad:  1.0 2.0 -4.08550288710785e-07\n",
            "\tgrad:  2.0 4.0 -1.6015171322436572e-06\n",
            "\tgrad:  3.0 6.0 -3.3151404608133817e-06\n",
            "progress: 51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
            "\tgrad:  1.0 2.0 -3.020461312175371e-07\n",
            "\tgrad:  2.0 4.0 -1.1840208351543424e-06\n",
            "\tgrad:  3.0 6.0 -2.4509231284497446e-06\n",
            "progress: 52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
            "\tgrad:  1.0 2.0 -2.2330632942768602e-07\n",
            "\tgrad:  2.0 4.0 -8.753608113920563e-07\n",
            "\tgrad:  3.0 6.0 -1.811996877876254e-06\n",
            "progress: 53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
            "\tgrad:  1.0 2.0 -1.6509304900935717e-07\n",
            "\tgrad:  2.0 4.0 -6.471647520100987e-07\n",
            "\tgrad:  3.0 6.0 -1.3396310407642886e-06\n",
            "progress: 54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
            "\tgrad:  1.0 2.0 -1.220552721115098e-07\n",
            "\tgrad:  2.0 4.0 -4.784566662863199e-07\n",
            "\tgrad:  3.0 6.0 -9.904052991061008e-07\n",
            "progress: 55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
            "\tgrad:  1.0 2.0 -9.023692726373156e-08\n",
            "\tgrad:  2.0 4.0 -3.5372875473171916e-07\n",
            "\tgrad:  3.0 6.0 -7.322185204827747e-07\n",
            "progress: 56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
            "\tgrad:  1.0 2.0 -6.671324292994996e-08\n",
            "\tgrad:  2.0 4.0 -2.615159129248923e-07\n",
            "\tgrad:  3.0 6.0 -5.413379398078177e-07\n",
            "progress: 57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
            "\tgrad:  1.0 2.0 -4.932190122985958e-08\n",
            "\tgrad:  2.0 4.0 -1.9334185274999527e-07\n",
            "\tgrad:  3.0 6.0 -4.002176350326181e-07\n",
            "progress: 58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
            "\tgrad:  1.0 2.0 -3.6464273378555845e-08\n",
            "\tgrad:  2.0 4.0 -1.429399514307761e-07\n",
            "\tgrad:  3.0 6.0 -2.9588569994132286e-07\n",
            "progress: 59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
            "\tgrad:  1.0 2.0 -2.6958475007887728e-08\n",
            "\tgrad:  2.0 4.0 -1.0567722164012139e-07\n",
            "\tgrad:  3.0 6.0 -2.1875184863517916e-07\n",
            "progress: 60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
            "\tgrad:  1.0 2.0 -1.993072418216002e-08\n",
            "\tgrad:  2.0 4.0 -7.812843882959442e-08\n",
            "\tgrad:  3.0 6.0 -1.617258700292723e-07\n",
            "progress: 61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
            "\tgrad:  1.0 2.0 -1.473502342363986e-08\n",
            "\tgrad:  2.0 4.0 -5.7761292637792394e-08\n",
            "\tgrad:  3.0 6.0 -1.195658771990793e-07\n",
            "progress: 62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
            "\tgrad:  1.0 2.0 -1.0893780100218464e-08\n",
            "\tgrad:  2.0 4.0 -4.270361841918202e-08\n",
            "\tgrad:  3.0 6.0 -8.839649012770678e-08\n",
            "progress: 63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
            "\tgrad:  1.0 2.0 -8.05390243385773e-09\n",
            "\tgrad:  2.0 4.0 -3.1571296688071016e-08\n",
            "\tgrad:  3.0 6.0 -6.53525820126788e-08\n",
            "progress: 64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
            "\tgrad:  1.0 2.0 -5.9543463493128e-09\n",
            "\tgrad:  2.0 4.0 -2.334103754719763e-08\n",
            "\tgrad:  3.0 6.0 -4.8315948575350376e-08\n",
            "progress: 65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
            "\tgrad:  1.0 2.0 -4.402119557767037e-09\n",
            "\tgrad:  2.0 4.0 -1.725630838222969e-08\n",
            "\tgrad:  3.0 6.0 -3.5720557178819945e-08\n",
            "progress: 66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
            "\tgrad:  1.0 2.0 -3.254539748809293e-09\n",
            "\tgrad:  2.0 4.0 -1.2757796596929438e-08\n",
            "\tgrad:  3.0 6.0 -2.6408640607655798e-08\n",
            "progress: 67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
            "\tgrad:  1.0 2.0 -2.406120636067044e-09\n",
            "\tgrad:  2.0 4.0 -9.431992964437086e-09\n",
            "\tgrad:  3.0 6.0 -1.9524227568012975e-08\n",
            "progress: 68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
            "\tgrad:  1.0 2.0 -1.7788739370416806e-09\n",
            "\tgrad:  2.0 4.0 -6.97318647269185e-09\n",
            "\tgrad:  3.0 6.0 -1.4434496264925656e-08\n",
            "progress: 69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
            "\tgrad:  1.0 2.0 -1.3151431055291596e-09\n",
            "\tgrad:  2.0 4.0 -5.155360582875801e-09\n",
            "\tgrad:  3.0 6.0 -1.067159693945996e-08\n",
            "progress: 70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
            "\tgrad:  1.0 2.0 -9.72300906454393e-10\n",
            "\tgrad:  2.0 4.0 -3.811418736177075e-09\n",
            "\tgrad:  3.0 6.0 -7.88963561149103e-09\n",
            "progress: 71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
            "\tgrad:  1.0 2.0 -7.18833437218791e-10\n",
            "\tgrad:  2.0 4.0 -2.8178277489132597e-09\n",
            "\tgrad:  3.0 6.0 -5.832902161273523e-09\n",
            "progress: 72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
            "\tgrad:  1.0 2.0 -5.314420015167798e-10\n",
            "\tgrad:  2.0 4.0 -2.0832526814729135e-09\n",
            "\tgrad:  3.0 6.0 -4.31233715403323e-09\n",
            "progress: 73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
            "\tgrad:  1.0 2.0 -3.92901711165905e-10\n",
            "\tgrad:  2.0 4.0 -1.5401742103904326e-09\n",
            "\tgrad:  3.0 6.0 -3.188159070077745e-09\n",
            "progress: 74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
            "\tgrad:  1.0 2.0 -2.9047697580608656e-10\n",
            "\tgrad:  2.0 4.0 -1.1386696030513122e-09\n",
            "\tgrad:  3.0 6.0 -2.3570478902001923e-09\n",
            "progress: 75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
            "\tgrad:  1.0 2.0 -2.1475310418850313e-10\n",
            "\tgrad:  2.0 4.0 -8.418314934033333e-10\n",
            "\tgrad:  3.0 6.0 -1.7425900722400911e-09\n",
            "progress: 76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
            "\tgrad:  1.0 2.0 -1.5876944203796484e-10\n",
            "\tgrad:  2.0 4.0 -6.223768167501476e-10\n",
            "\tgrad:  3.0 6.0 -1.2883241140571045e-09\n",
            "progress: 77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
            "\tgrad:  1.0 2.0 -1.17380327679939e-10\n",
            "\tgrad:  2.0 4.0 -4.601314884666863e-10\n",
            "\tgrad:  3.0 6.0 -9.524754318590567e-10\n",
            "progress: 78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
            "\tgrad:  1.0 2.0 -8.678080476443029e-11\n",
            "\tgrad:  2.0 4.0 -3.4018121652934497e-10\n",
            "\tgrad:  3.0 6.0 -7.041780492045291e-10\n",
            "progress: 79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
            "\tgrad:  1.0 2.0 -6.415845632545825e-11\n",
            "\tgrad:  2.0 4.0 -2.5150193039280566e-10\n",
            "\tgrad:  3.0 6.0 -5.206075570640678e-10\n",
            "progress: 80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
            "\tgrad:  1.0 2.0 -4.743316850408519e-11\n",
            "\tgrad:  2.0 4.0 -1.8593837580738182e-10\n",
            "\tgrad:  3.0 6.0 -3.8489211817704927e-10\n",
            "progress: 81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
            "\tgrad:  1.0 2.0 -3.5067948545020045e-11\n",
            "\tgrad:  2.0 4.0 -1.3746692673066718e-10\n",
            "\tgrad:  3.0 6.0 -2.845563784603655e-10\n",
            "progress: 82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
            "\tgrad:  1.0 2.0 -2.5926372160256506e-11\n",
            "\tgrad:  2.0 4.0 -1.0163070385260653e-10\n",
            "\tgrad:  3.0 6.0 -2.1037571684701106e-10\n",
            "progress: 83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
            "\tgrad:  1.0 2.0 -1.9167778475548403e-11\n",
            "\tgrad:  2.0 4.0 -7.51381179497912e-11\n",
            "\tgrad:  3.0 6.0 -1.5553425214420713e-10\n",
            "progress: 84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
            "\tgrad:  1.0 2.0 -1.4170886686315498e-11\n",
            "\tgrad:  2.0 4.0 -5.555023108172463e-11\n",
            "\tgrad:  3.0 6.0 -1.1499068364173581e-10\n",
            "progress: 85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
            "\tgrad:  1.0 2.0 -1.0476508549572827e-11\n",
            "\tgrad:  2.0 4.0 -4.106759377009439e-11\n",
            "\tgrad:  3.0 6.0 -8.500933290633839e-11\n",
            "progress: 86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
            "\tgrad:  1.0 2.0 -7.745359908994942e-12\n",
            "\tgrad:  2.0 4.0 -3.036149109902908e-11\n",
            "\tgrad:  3.0 6.0 -6.285105769165966e-11\n",
            "progress: 87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
            "\tgrad:  1.0 2.0 -5.726086271806707e-12\n",
            "\tgrad:  2.0 4.0 -2.2446045022661565e-11\n",
            "\tgrad:  3.0 6.0 -4.646416584819235e-11\n",
            "progress: 88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
            "\tgrad:  1.0 2.0 -4.233058348290797e-12\n",
            "\tgrad:  2.0 4.0 -1.659294923683774e-11\n",
            "\tgrad:  3.0 6.0 -3.4351188560322043e-11\n",
            "progress: 89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
            "\tgrad:  1.0 2.0 -3.1294966618133913e-12\n",
            "\tgrad:  2.0 4.0 -1.226752033289813e-11\n",
            "\tgrad:  3.0 6.0 -2.539835008974478e-11\n",
            "progress: 90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
            "\tgrad:  1.0 2.0 -2.3137047833188262e-12\n",
            "\tgrad:  2.0 4.0 -9.070078021977679e-12\n",
            "\tgrad:  3.0 6.0 -1.8779644506139448e-11\n",
            "progress: 91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
            "\tgrad:  1.0 2.0 -1.7106316363424412e-12\n",
            "\tgrad:  2.0 4.0 -6.7057470687359455e-12\n",
            "\tgrad:  3.0 6.0 -1.3882228699912957e-11\n",
            "progress: 92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
            "\tgrad:  1.0 2.0 -1.2647660696529783e-12\n",
            "\tgrad:  2.0 4.0 -4.957811938766099e-12\n",
            "\tgrad:  3.0 6.0 -1.0263789818054647e-11\n",
            "progress: 93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
            "\tgrad:  1.0 2.0 -9.352518759442319e-13\n",
            "\tgrad:  2.0 4.0 -3.666400516522117e-12\n",
            "\tgrad:  3.0 6.0 -7.58859641791787e-12\n",
            "progress: 94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
            "\tgrad:  1.0 2.0 -6.914468997365475e-13\n",
            "\tgrad:  2.0 4.0 -2.7107205369247822e-12\n",
            "\tgrad:  3.0 6.0 -5.611511255665391e-12\n",
            "progress: 95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
            "\tgrad:  1.0 2.0 -5.111466805374221e-13\n",
            "\tgrad:  2.0 4.0 -2.0037305148434825e-12\n",
            "\tgrad:  3.0 6.0 -4.1460168631601846e-12\n",
            "progress: 96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
            "\tgrad:  1.0 2.0 -3.779199175824033e-13\n",
            "\tgrad:  2.0 4.0 -1.4814816040598089e-12\n",
            "\tgrad:  3.0 6.0 -3.064215547965432e-12\n",
            "progress: 97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
            "\tgrad:  1.0 2.0 -2.793321129956894e-13\n",
            "\tgrad:  2.0 4.0 -1.0942358130705543e-12\n",
            "\tgrad:  3.0 6.0 -2.2648549702353193e-12\n",
            "progress: 98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
            "\tgrad:  1.0 2.0 -2.0650148258027912e-13\n",
            "\tgrad:  2.0 4.0 -8.100187187665142e-13\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress: 99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
            "predict (after training) 4 hours 7.9999999999996945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P42McuY4ISz"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlA4Xd0L4apW",
        "outputId": "d1a5c366-cfdd-4e17-92cb-19b55a5c7fcb"
      },
      "source": [
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "   return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "   y_pred = forward(x)\n",
        "   return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# Before training\n",
        "print(\"predict (before training)\",  4, forward(4).data[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor(4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kObIKBzM4f-K",
        "outputId": "f324772a-1113-4bcf-c431-60241c636dba"
      },
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "   for x_val, y_val in zip(x_data, y_data):\n",
        "       l = loss(x_val, y_val)\n",
        "       l.backward()\n",
        "       print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
        "       w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "       # Manually zero the gradients after updating weights\n",
        "       w.grad.data.zero_()\n",
        "\n",
        "   print(\"progress:\", epoch, l.data[0])\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\",  4, forward(4).data[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tgrad:  1.0 2.0 tensor(-2.)\n",
            "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
            "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
            "progress: 0 tensor(7.3159)\n",
            "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
            "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
            "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
            "progress: 1 tensor(3.9988)\n",
            "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
            "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
            "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
            "progress: 2 tensor(2.1857)\n",
            "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
            "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
            "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
            "progress: 3 tensor(1.1946)\n",
            "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
            "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
            "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
            "progress: 4 tensor(0.6530)\n",
            "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
            "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
            "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
            "progress: 5 tensor(0.3569)\n",
            "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
            "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
            "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
            "progress: 6 tensor(0.1951)\n",
            "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
            "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
            "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
            "progress: 7 tensor(0.1066)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
            "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
            "progress: 8 tensor(0.0583)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
            "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
            "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
            "progress: 9 tensor(0.0319)\n",
            "predict (after training) 4 tensor(7.8049)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOwEaP4G6W85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7efae0-3af8-42b1-9ec3-7320e7b73e80"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 53.13786315917969 \n",
            "Epoch: 1 | Loss: 23.672096252441406 \n",
            "Epoch: 2 | Loss: 10.554533004760742 \n",
            "Epoch: 3 | Loss: 4.714731216430664 \n",
            "Epoch: 4 | Loss: 2.1147851943969727 \n",
            "Epoch: 5 | Loss: 0.9571337699890137 \n",
            "Epoch: 6 | Loss: 0.44155430793762207 \n",
            "Epoch: 7 | Loss: 0.21181075274944305 \n",
            "Epoch: 8 | Loss: 0.109316885471344 \n",
            "Epoch: 9 | Loss: 0.06347334384918213 \n",
            "Epoch: 10 | Loss: 0.0428522452712059 \n",
            "Epoch: 11 | Loss: 0.03346260264515877 \n",
            "Epoch: 12 | Loss: 0.02907596156001091 \n",
            "Epoch: 13 | Loss: 0.026919318363070488 \n",
            "Epoch: 14 | Loss: 0.02575843222439289 \n",
            "Epoch: 15 | Loss: 0.025043634697794914 \n",
            "Epoch: 16 | Loss: 0.02453039027750492 \n",
            "Epoch: 17 | Loss: 0.024109549820423126 \n",
            "Epoch: 18 | Loss: 0.023732608184218407 \n",
            "Epoch: 19 | Loss: 0.023378072306513786 \n",
            "Epoch: 20 | Loss: 0.023036014288663864 \n",
            "Epoch: 21 | Loss: 0.02270224317908287 \n",
            "Epoch: 22 | Loss: 0.02237481065094471 \n",
            "Epoch: 23 | Loss: 0.022052666172385216 \n",
            "Epoch: 24 | Loss: 0.021735507994890213 \n",
            "Epoch: 25 | Loss: 0.021423012018203735 \n",
            "Epoch: 26 | Loss: 0.021115107461810112 \n",
            "Epoch: 27 | Loss: 0.020811665803194046 \n",
            "Epoch: 28 | Loss: 0.020512519404292107 \n",
            "Epoch: 29 | Loss: 0.020217735320329666 \n",
            "Epoch: 30 | Loss: 0.019927145913243294 \n",
            "Epoch: 31 | Loss: 0.019640792161226273 \n",
            "Epoch: 32 | Loss: 0.01935848779976368 \n",
            "Epoch: 33 | Loss: 0.01908029243350029 \n",
            "Epoch: 34 | Loss: 0.018806062638759613 \n",
            "Epoch: 35 | Loss: 0.018535804003477097 \n",
            "Epoch: 36 | Loss: 0.018269484862685204 \n",
            "Epoch: 37 | Loss: 0.01800684444606304 \n",
            "Epoch: 38 | Loss: 0.017748041078448296 \n",
            "Epoch: 39 | Loss: 0.017493009567260742 \n",
            "Epoch: 40 | Loss: 0.017241591587662697 \n",
            "Epoch: 41 | Loss: 0.01699381321668625 \n",
            "Epoch: 42 | Loss: 0.016749560832977295 \n",
            "Epoch: 43 | Loss: 0.01650882139801979 \n",
            "Epoch: 44 | Loss: 0.01627160981297493 \n",
            "Epoch: 45 | Loss: 0.016037750989198685 \n",
            "Epoch: 46 | Loss: 0.015807323157787323 \n",
            "Epoch: 47 | Loss: 0.015580041334033012 \n",
            "Epoch: 48 | Loss: 0.01535615511238575 \n",
            "Epoch: 49 | Loss: 0.015135502442717552 \n",
            "Epoch: 50 | Loss: 0.014917941763997078 \n",
            "Epoch: 51 | Loss: 0.014703595079481602 \n",
            "Epoch: 52 | Loss: 0.014492248184978962 \n",
            "Epoch: 53 | Loss: 0.014284023083746433 \n",
            "Epoch: 54 | Loss: 0.014078705571591854 \n",
            "Epoch: 55 | Loss: 0.013876366429030895 \n",
            "Epoch: 56 | Loss: 0.01367693766951561 \n",
            "Epoch: 57 | Loss: 0.013480381108820438 \n",
            "Epoch: 58 | Loss: 0.013286689296364784 \n",
            "Epoch: 59 | Loss: 0.013095655478537083 \n",
            "Epoch: 60 | Loss: 0.012907523661851883 \n",
            "Epoch: 61 | Loss: 0.012722020968794823 \n",
            "Epoch: 62 | Loss: 0.012539179995656013 \n",
            "Epoch: 63 | Loss: 0.012358971871435642 \n",
            "Epoch: 64 | Loss: 0.012181325815618038 \n",
            "Epoch: 65 | Loss: 0.012006256729364395 \n",
            "Epoch: 66 | Loss: 0.01183369942009449 \n",
            "Epoch: 67 | Loss: 0.011663652956485748 \n",
            "Epoch: 68 | Loss: 0.011496048420667648 \n",
            "Epoch: 69 | Loss: 0.011330818757414818 \n",
            "Epoch: 70 | Loss: 0.01116795651614666 \n",
            "Epoch: 71 | Loss: 0.011007452383637428 \n",
            "Epoch: 72 | Loss: 0.010849276557564735 \n",
            "Epoch: 73 | Loss: 0.010693364776670933 \n",
            "Epoch: 74 | Loss: 0.010539726354181767 \n",
            "Epoch: 75 | Loss: 0.010388154536485672 \n",
            "Epoch: 76 | Loss: 0.01023890357464552 \n",
            "Epoch: 77 | Loss: 0.010091770440340042 \n",
            "Epoch: 78 | Loss: 0.0099467309191823 \n",
            "Epoch: 79 | Loss: 0.009803771041333675 \n",
            "Epoch: 80 | Loss: 0.00966288335621357 \n",
            "Epoch: 81 | Loss: 0.009524049237370491 \n",
            "Epoch: 82 | Loss: 0.009387163445353508 \n",
            "Epoch: 83 | Loss: 0.009252265095710754 \n",
            "Epoch: 84 | Loss: 0.009119279682636261 \n",
            "Epoch: 85 | Loss: 0.008988224901258945 \n",
            "Epoch: 86 | Loss: 0.008859031833708286 \n",
            "Epoch: 87 | Loss: 0.008731708861887455 \n",
            "Epoch: 88 | Loss: 0.00860628392547369 \n",
            "Epoch: 89 | Loss: 0.008482515811920166 \n",
            "Epoch: 90 | Loss: 0.00836062990128994 \n",
            "Epoch: 91 | Loss: 0.008240492083132267 \n",
            "Epoch: 92 | Loss: 0.00812201201915741 \n",
            "Epoch: 93 | Loss: 0.008005298674106598 \n",
            "Epoch: 94 | Loss: 0.007890273816883564 \n",
            "Epoch: 95 | Loss: 0.007776885759085417 \n",
            "Epoch: 96 | Loss: 0.007665123324841261 \n",
            "Epoch: 97 | Loss: 0.007554938085377216 \n",
            "Epoch: 98 | Loss: 0.0074463533237576485 \n",
            "Epoch: 99 | Loss: 0.0073393564671278 \n",
            "Epoch: 100 | Loss: 0.007233877666294575 \n",
            "Epoch: 101 | Loss: 0.00712993647903204 \n",
            "Epoch: 102 | Loss: 0.007027409505099058 \n",
            "Epoch: 103 | Loss: 0.006926443427801132 \n",
            "Epoch: 104 | Loss: 0.006826887372881174 \n",
            "Epoch: 105 | Loss: 0.006728784646838903 \n",
            "Epoch: 106 | Loss: 0.006632068194448948 \n",
            "Epoch: 107 | Loss: 0.006536734290421009 \n",
            "Epoch: 108 | Loss: 0.00644278759136796 \n",
            "Epoch: 109 | Loss: 0.006350213661789894 \n",
            "Epoch: 110 | Loss: 0.006258944980800152 \n",
            "Epoch: 111 | Loss: 0.006168985273689032 \n",
            "Epoch: 112 | Loss: 0.00608033686876297 \n",
            "Epoch: 113 | Loss: 0.005992959253489971 \n",
            "Epoch: 114 | Loss: 0.005906802136451006 \n",
            "Epoch: 115 | Loss: 0.005821903236210346 \n",
            "Epoch: 116 | Loss: 0.0057382439263165 \n",
            "Epoch: 117 | Loss: 0.005655794404447079 \n",
            "Epoch: 118 | Loss: 0.005574520211666822 \n",
            "Epoch: 119 | Loss: 0.005494399461895227 \n",
            "Epoch: 120 | Loss: 0.0054154423996806145 \n",
            "Epoch: 121 | Loss: 0.005337579175829887 \n",
            "Epoch: 122 | Loss: 0.0052609192207455635 \n",
            "Epoch: 123 | Loss: 0.005185281857848167 \n",
            "Epoch: 124 | Loss: 0.0051107704639434814 \n",
            "Epoch: 125 | Loss: 0.005037311930209398 \n",
            "Epoch: 126 | Loss: 0.004964903462678194 \n",
            "Epoch: 127 | Loss: 0.004893550183624029 \n",
            "Epoch: 128 | Loss: 0.004823248367756605 \n",
            "Epoch: 129 | Loss: 0.004753910005092621 \n",
            "Epoch: 130 | Loss: 0.0046855974942445755 \n",
            "Epoch: 131 | Loss: 0.0046182963997125626 \n",
            "Epoch: 132 | Loss: 0.004551882389932871 \n",
            "Epoch: 133 | Loss: 0.004486494697630405 \n",
            "Epoch: 134 | Loss: 0.004421986173838377 \n",
            "Epoch: 135 | Loss: 0.004358437843620777 \n",
            "Epoch: 136 | Loss: 0.004295786377042532 \n",
            "Epoch: 137 | Loss: 0.0042340680956840515 \n",
            "Epoch: 138 | Loss: 0.004173221532255411 \n",
            "Epoch: 139 | Loss: 0.004113227128982544 \n",
            "Epoch: 140 | Loss: 0.004054115153849125 \n",
            "Epoch: 141 | Loss: 0.003995879553258419 \n",
            "Epoch: 142 | Loss: 0.003938451875001192 \n",
            "Epoch: 143 | Loss: 0.0038818581961095333 \n",
            "Epoch: 144 | Loss: 0.003826068015769124 \n",
            "Epoch: 145 | Loss: 0.003771065501496196 \n",
            "Epoch: 146 | Loss: 0.003716870676726103 \n",
            "Epoch: 147 | Loss: 0.0036634725984185934 \n",
            "Epoch: 148 | Loss: 0.0036108035128563643 \n",
            "Epoch: 149 | Loss: 0.0035589165054261684 \n",
            "Epoch: 150 | Loss: 0.0035077680367976427 \n",
            "Epoch: 151 | Loss: 0.003457355545833707 \n",
            "Epoch: 152 | Loss: 0.0034076832234859467 \n",
            "Epoch: 153 | Loss: 0.0033586889039725065 \n",
            "Epoch: 154 | Loss: 0.003310438944026828 \n",
            "Epoch: 155 | Loss: 0.0032628350891172886 \n",
            "Epoch: 156 | Loss: 0.003215950448065996 \n",
            "Epoch: 157 | Loss: 0.00316972890868783 \n",
            "Epoch: 158 | Loss: 0.003124173963442445 \n",
            "Epoch: 159 | Loss: 0.0030792774632573128 \n",
            "Epoch: 160 | Loss: 0.003035018453374505 \n",
            "Epoch: 161 | Loss: 0.0029913880862295628 \n",
            "Epoch: 162 | Loss: 0.0029484101105481386 \n",
            "Epoch: 163 | Loss: 0.0029060624074190855 \n",
            "Epoch: 164 | Loss: 0.002864263951778412 \n",
            "Epoch: 165 | Loss: 0.0028231267351657152 \n",
            "Epoch: 166 | Loss: 0.0027825506404042244 \n",
            "Epoch: 167 | Loss: 0.002742559416219592 \n",
            "Epoch: 168 | Loss: 0.002703144447878003 \n",
            "Epoch: 169 | Loss: 0.002664310624822974 \n",
            "Epoch: 170 | Loss: 0.0026260081212967634 \n",
            "Epoch: 171 | Loss: 0.002588254865258932 \n",
            "Epoch: 172 | Loss: 0.0025510541163384914 \n",
            "Epoch: 173 | Loss: 0.0025143807288259268 \n",
            "Epoch: 174 | Loss: 0.00247825775295496 \n",
            "Epoch: 175 | Loss: 0.0024426563177257776 \n",
            "Epoch: 176 | Loss: 0.0024075692053884268 \n",
            "Epoch: 177 | Loss: 0.0023729540407657623 \n",
            "Epoch: 178 | Loss: 0.0023388525005429983 \n",
            "Epoch: 179 | Loss: 0.0023052492178976536 \n",
            "Epoch: 180 | Loss: 0.0022721034474670887 \n",
            "Epoch: 181 | Loss: 0.002239435911178589 \n",
            "Epoch: 182 | Loss: 0.0022072724532335997 \n",
            "Epoch: 183 | Loss: 0.002175533212721348 \n",
            "Epoch: 184 | Loss: 0.0021442833822220564 \n",
            "Epoch: 185 | Loss: 0.002113469410687685 \n",
            "Epoch: 186 | Loss: 0.002083073602989316 \n",
            "Epoch: 187 | Loss: 0.002053161384537816 \n",
            "Epoch: 188 | Loss: 0.002023648703470826 \n",
            "Epoch: 189 | Loss: 0.0019945509266108274 \n",
            "Epoch: 190 | Loss: 0.0019658892415463924 \n",
            "Epoch: 191 | Loss: 0.0019376232521608472 \n",
            "Epoch: 192 | Loss: 0.0019097905606031418 \n",
            "Epoch: 193 | Loss: 0.0018823560094460845 \n",
            "Epoch: 194 | Loss: 0.0018552965484559536 \n",
            "Epoch: 195 | Loss: 0.0018286318518221378 \n",
            "Epoch: 196 | Loss: 0.0018023527227342129 \n",
            "Epoch: 197 | Loss: 0.0017764352960512042 \n",
            "Epoch: 198 | Loss: 0.0017509087920188904 \n",
            "Epoch: 199 | Loss: 0.0017257553990930319 \n",
            "Epoch: 200 | Loss: 0.001700929133221507 \n",
            "Epoch: 201 | Loss: 0.0016764909960329533 \n",
            "Epoch: 202 | Loss: 0.0016524175880476832 \n",
            "Epoch: 203 | Loss: 0.0016286532627418637 \n",
            "Epoch: 204 | Loss: 0.0016052693827077746 \n",
            "Epoch: 205 | Loss: 0.0015821934211999178 \n",
            "Epoch: 206 | Loss: 0.001559463795274496 \n",
            "Epoch: 207 | Loss: 0.001537031726911664 \n",
            "Epoch: 208 | Loss: 0.0015149491373449564 \n",
            "Epoch: 209 | Loss: 0.0014931653859093785 \n",
            "Epoch: 210 | Loss: 0.0014717057347297668 \n",
            "Epoch: 211 | Loss: 0.0014505707658827305 \n",
            "Epoch: 212 | Loss: 0.0014297193847596645 \n",
            "Epoch: 213 | Loss: 0.0014091555494815111 \n",
            "Epoch: 214 | Loss: 0.0013889215188100934 \n",
            "Epoch: 215 | Loss: 0.001368956407532096 \n",
            "Epoch: 216 | Loss: 0.001349271391518414 \n",
            "Epoch: 217 | Loss: 0.0013298934791237116 \n",
            "Epoch: 218 | Loss: 0.0013107681879773736 \n",
            "Epoch: 219 | Loss: 0.0012919360306113958 \n",
            "Epoch: 220 | Loss: 0.001273368950933218 \n",
            "Epoch: 221 | Loss: 0.0012550721876323223 \n",
            "Epoch: 222 | Loss: 0.00123702734708786 \n",
            "Epoch: 223 | Loss: 0.0012192431604489684 \n",
            "Epoch: 224 | Loss: 0.001201721839606762 \n",
            "Epoch: 225 | Loss: 0.0011844532564282417 \n",
            "Epoch: 226 | Loss: 0.001167444745078683 \n",
            "Epoch: 227 | Loss: 0.0011506639420986176 \n",
            "Epoch: 228 | Loss: 0.0011341434437781572 \n",
            "Epoch: 229 | Loss: 0.0011178338900208473 \n",
            "Epoch: 230 | Loss: 0.00110177556052804 \n",
            "Epoch: 231 | Loss: 0.0010859293397516012 \n",
            "Epoch: 232 | Loss: 0.0010703176958486438 \n",
            "Epoch: 233 | Loss: 0.0010549465660005808 \n",
            "Epoch: 234 | Loss: 0.0010397803271189332 \n",
            "Epoch: 235 | Loss: 0.0010248429607599974 \n",
            "Epoch: 236 | Loss: 0.0010101089719682932 \n",
            "Epoch: 237 | Loss: 0.0009955880232155323 \n",
            "Epoch: 238 | Loss: 0.0009812713833525777 \n",
            "Epoch: 239 | Loss: 0.0009671715670265257 \n",
            "Epoch: 240 | Loss: 0.0009532778058201075 \n",
            "Epoch: 241 | Loss: 0.0009395701345056295 \n",
            "Epoch: 242 | Loss: 0.0009260812075808644 \n",
            "Epoch: 243 | Loss: 0.0009127600933425128 \n",
            "Epoch: 244 | Loss: 0.0008996418328024447 \n",
            "Epoch: 245 | Loss: 0.0008867207216098905 \n",
            "Epoch: 246 | Loss: 0.0008739703334867954 \n",
            "Epoch: 247 | Loss: 0.0008614164544269443 \n",
            "Epoch: 248 | Loss: 0.0008490338223055005 \n",
            "Epoch: 249 | Loss: 0.0008368409471586347 \n",
            "Epoch: 250 | Loss: 0.000824811402708292 \n",
            "Epoch: 251 | Loss: 0.0008129452471621335 \n",
            "Epoch: 252 | Loss: 0.0008012728067114949 \n",
            "Epoch: 253 | Loss: 0.0007897468749433756 \n",
            "Epoch: 254 | Loss: 0.0007784139015711844 \n",
            "Epoch: 255 | Loss: 0.0007672111969441175 \n",
            "Epoch: 256 | Loss: 0.0007562048267573118 \n",
            "Epoch: 257 | Loss: 0.0007453232537955046 \n",
            "Epoch: 258 | Loss: 0.0007346158381551504 \n",
            "Epoch: 259 | Loss: 0.0007240429986268282 \n",
            "Epoch: 260 | Loss: 0.0007136430940590799 \n",
            "Epoch: 261 | Loss: 0.0007033870206214488 \n",
            "Epoch: 262 | Loss: 0.0006932886317372322 \n",
            "Epoch: 263 | Loss: 0.0006833202787674963 \n",
            "Epoch: 264 | Loss: 0.0006734943017363548 \n",
            "Epoch: 265 | Loss: 0.0006638270569965243 \n",
            "Epoch: 266 | Loss: 0.0006542914779856801 \n",
            "Epoch: 267 | Loss: 0.0006448713829740882 \n",
            "Epoch: 268 | Loss: 0.0006356053054332733 \n",
            "Epoch: 269 | Loss: 0.0006264732219278812 \n",
            "Epoch: 270 | Loss: 0.0006174694281071424 \n",
            "Epoch: 271 | Loss: 0.0006085938075557351 \n",
            "Epoch: 272 | Loss: 0.0005998522974550724 \n",
            "Epoch: 273 | Loss: 0.0005912213819101453 \n",
            "Epoch: 274 | Loss: 0.0005827351124025881 \n",
            "Epoch: 275 | Loss: 0.0005743540823459625 \n",
            "Epoch: 276 | Loss: 0.0005661057075485587 \n",
            "Epoch: 277 | Loss: 0.0005579533753916621 \n",
            "Epoch: 278 | Loss: 0.0005499467370100319 \n",
            "Epoch: 279 | Loss: 0.0005420446977950633 \n",
            "Epoch: 280 | Loss: 0.0005342461518011987 \n",
            "Epoch: 281 | Loss: 0.0005265655927360058 \n",
            "Epoch: 282 | Loss: 0.0005190001102164388 \n",
            "Epoch: 283 | Loss: 0.0005115545354783535 \n",
            "Epoch: 284 | Loss: 0.000504201278090477 \n",
            "Epoch: 285 | Loss: 0.0004969537840224802 \n",
            "Epoch: 286 | Loss: 0.0004898114711977541 \n",
            "Epoch: 287 | Loss: 0.00048277771566063166 \n",
            "Epoch: 288 | Loss: 0.0004758240538649261 \n",
            "Epoch: 289 | Loss: 0.0004689956549555063 \n",
            "Epoch: 290 | Loss: 0.00046224979450926185 \n",
            "Epoch: 291 | Loss: 0.00045560923172160983 \n",
            "Epoch: 292 | Loss: 0.0004490673018153757 \n",
            "Epoch: 293 | Loss: 0.0004426087252795696 \n",
            "Epoch: 294 | Loss: 0.00043625221587717533 \n",
            "Epoch: 295 | Loss: 0.0004299771972000599 \n",
            "Epoch: 296 | Loss: 0.0004237979301251471 \n",
            "Epoch: 297 | Loss: 0.0004177079536020756 \n",
            "Epoch: 298 | Loss: 0.0004117010103072971 \n",
            "Epoch: 299 | Loss: 0.0004057878686580807 \n",
            "Epoch: 300 | Loss: 0.0003999489708803594 \n",
            "Epoch: 301 | Loss: 0.00039420370012521744 \n",
            "Epoch: 302 | Loss: 0.00038854015292599797 \n",
            "Epoch: 303 | Loss: 0.000382959668058902 \n",
            "Epoch: 304 | Loss: 0.000377457938157022 \n",
            "Epoch: 305 | Loss: 0.00037203248939476907 \n",
            "Epoch: 306 | Loss: 0.0003666863776743412 \n",
            "Epoch: 307 | Loss: 0.00036142152384854853 \n",
            "Epoch: 308 | Loss: 0.0003562167112249881 \n",
            "Epoch: 309 | Loss: 0.0003510986571200192 \n",
            "Epoch: 310 | Loss: 0.00034605656401254237 \n",
            "Epoch: 311 | Loss: 0.00034107835381291807 \n",
            "Epoch: 312 | Loss: 0.0003361814597155899 \n",
            "Epoch: 313 | Loss: 0.00033134385012090206 \n",
            "Epoch: 314 | Loss: 0.00032658016425557435 \n",
            "Epoch: 315 | Loss: 0.00032189994817599654 \n",
            "Epoch: 316 | Loss: 0.00031726702582091093 \n",
            "Epoch: 317 | Loss: 0.000312709336867556 \n",
            "Epoch: 318 | Loss: 0.00030822440749034286 \n",
            "Epoch: 319 | Loss: 0.00030378811061382294 \n",
            "Epoch: 320 | Loss: 0.00029941313550807536 \n",
            "Epoch: 321 | Loss: 0.00029511682805605233 \n",
            "Epoch: 322 | Loss: 0.0002908734022639692 \n",
            "Epoch: 323 | Loss: 0.0002866931608878076 \n",
            "Epoch: 324 | Loss: 0.00028257397934794426 \n",
            "Epoch: 325 | Loss: 0.00027851323829963803 \n",
            "Epoch: 326 | Loss: 0.00027450695051811635 \n",
            "Epoch: 327 | Loss: 0.00027056504040956497 \n",
            "Epoch: 328 | Loss: 0.0002666820655576885 \n",
            "Epoch: 329 | Loss: 0.0002628497313708067 \n",
            "Epoch: 330 | Loss: 0.00025906949304044247 \n",
            "Epoch: 331 | Loss: 0.00025533707230351865 \n",
            "Epoch: 332 | Loss: 0.00025167371495626867 \n",
            "Epoch: 333 | Loss: 0.0002480587863828987 \n",
            "Epoch: 334 | Loss: 0.00024450093042105436 \n",
            "Epoch: 335 | Loss: 0.00024098006542772055 \n",
            "Epoch: 336 | Loss: 0.00023751921253278852 \n",
            "Epoch: 337 | Loss: 0.0002340994105907157 \n",
            "Epoch: 338 | Loss: 0.00023073797638062388 \n",
            "Epoch: 339 | Loss: 0.00022741926659364253 \n",
            "Epoch: 340 | Loss: 0.00022415243438445032 \n",
            "Epoch: 341 | Loss: 0.00022092871950007975 \n",
            "Epoch: 342 | Loss: 0.00021775448112748563 \n",
            "Epoch: 343 | Loss: 0.00021462749282363802 \n",
            "Epoch: 344 | Loss: 0.00021154090063646436 \n",
            "Epoch: 345 | Loss: 0.0002085035084746778 \n",
            "Epoch: 346 | Loss: 0.00020550269982777536 \n",
            "Epoch: 347 | Loss: 0.00020254886476323009 \n",
            "Epoch: 348 | Loss: 0.00019964147941209376 \n",
            "Epoch: 349 | Loss: 0.0001967742427950725 \n",
            "Epoch: 350 | Loss: 0.00019394600531086326 \n",
            "Epoch: 351 | Loss: 0.00019115871691610664 \n",
            "Epoch: 352 | Loss: 0.00018841595738194883 \n",
            "Epoch: 353 | Loss: 0.00018569895473774523 \n",
            "Epoch: 354 | Loss: 0.00018302920216228813 \n",
            "Epoch: 355 | Loss: 0.00018040768918581307 \n",
            "Epoch: 356 | Loss: 0.00017781245696824044 \n",
            "Epoch: 357 | Loss: 0.00017525834846310318 \n",
            "Epoch: 358 | Loss: 0.00017273915000259876 \n",
            "Epoch: 359 | Loss: 0.00017025298438966274 \n",
            "Epoch: 360 | Loss: 0.0001678100525168702 \n",
            "Epoch: 361 | Loss: 0.0001653963845456019 \n",
            "Epoch: 362 | Loss: 0.0001630180049687624 \n",
            "Epoch: 363 | Loss: 0.00016067529213614762 \n",
            "Epoch: 364 | Loss: 0.0001583670818945393 \n",
            "Epoch: 365 | Loss: 0.00015608937246724963 \n",
            "Epoch: 366 | Loss: 0.00015384727157652378 \n",
            "Epoch: 367 | Loss: 0.00015163786883931607 \n",
            "Epoch: 368 | Loss: 0.00014945190923754126 \n",
            "Epoch: 369 | Loss: 0.00014730298426002264 \n",
            "Epoch: 370 | Loss: 0.00014519414980895817 \n",
            "Epoch: 371 | Loss: 0.00014310919505078346 \n",
            "Epoch: 372 | Loss: 0.00014104487490840256 \n",
            "Epoch: 373 | Loss: 0.00013902020873501897 \n",
            "Epoch: 374 | Loss: 0.00013702106662094593 \n",
            "Epoch: 375 | Loss: 0.00013505667448043823 \n",
            "Epoch: 376 | Loss: 0.00013311486691236496 \n",
            "Epoch: 377 | Loss: 0.00013119744835421443 \n",
            "Epoch: 378 | Loss: 0.00012931176752317697 \n",
            "Epoch: 379 | Loss: 0.0001274542009923607 \n",
            "Epoch: 380 | Loss: 0.00012561821495182812 \n",
            "Epoch: 381 | Loss: 0.00012381882697809488 \n",
            "Epoch: 382 | Loss: 0.00012203659571241587 \n",
            "Epoch: 383 | Loss: 0.00012028323544654995 \n",
            "Epoch: 384 | Loss: 0.00011855742195621133 \n",
            "Epoch: 385 | Loss: 0.00011685101344482973 \n",
            "Epoch: 386 | Loss: 0.00011517257371451706 \n",
            "Epoch: 387 | Loss: 0.00011351618741173297 \n",
            "Epoch: 388 | Loss: 0.00011188593634869903 \n",
            "Epoch: 389 | Loss: 0.00011027633445337415 \n",
            "Epoch: 390 | Loss: 0.00010869233665289357 \n",
            "Epoch: 391 | Loss: 0.00010712822404457256 \n",
            "Epoch: 392 | Loss: 0.00010559546353761107 \n",
            "Epoch: 393 | Loss: 0.0001040752322296612 \n",
            "Epoch: 394 | Loss: 0.00010257984831696376 \n",
            "Epoch: 395 | Loss: 0.00010110143921338022 \n",
            "Epoch: 396 | Loss: 9.964942728402093e-05 \n",
            "Epoch: 397 | Loss: 9.821655839914456e-05 \n",
            "Epoch: 398 | Loss: 9.680951188784093e-05 \n",
            "Epoch: 399 | Loss: 9.541972394799814e-05 \n",
            "Epoch: 400 | Loss: 9.404421143699437e-05 \n",
            "Epoch: 401 | Loss: 9.269072324968874e-05 \n",
            "Epoch: 402 | Loss: 9.135648724623024e-05 \n",
            "Epoch: 403 | Loss: 9.005123138194904e-05 \n",
            "Epoch: 404 | Loss: 8.875592902768403e-05 \n",
            "Epoch: 405 | Loss: 8.74808756634593e-05 \n",
            "Epoch: 406 | Loss: 8.621560118626803e-05 \n",
            "Epoch: 407 | Loss: 8.498149691149592e-05 \n",
            "Epoch: 408 | Loss: 8.376028563361615e-05 \n",
            "Epoch: 409 | Loss: 8.25561219244264e-05 \n",
            "Epoch: 410 | Loss: 8.13719307188876e-05 \n",
            "Epoch: 411 | Loss: 8.02002105046995e-05 \n",
            "Epoch: 412 | Loss: 7.904913218226284e-05 \n",
            "Epoch: 413 | Loss: 7.79133042669855e-05 \n",
            "Epoch: 414 | Loss: 7.679358532186598e-05 \n",
            "Epoch: 415 | Loss: 7.56880544940941e-05 \n",
            "Epoch: 416 | Loss: 7.459906919393688e-05 \n",
            "Epoch: 417 | Loss: 7.352719694608822e-05 \n",
            "Epoch: 418 | Loss: 7.246927998494357e-05 \n",
            "Epoch: 419 | Loss: 7.142811227822676e-05 \n",
            "Epoch: 420 | Loss: 7.040008495096117e-05 \n",
            "Epoch: 421 | Loss: 6.939403101569042e-05 \n",
            "Epoch: 422 | Loss: 6.839499837951735e-05 \n",
            "Epoch: 423 | Loss: 6.740988465026021e-05 \n",
            "Epoch: 424 | Loss: 6.644590030191466e-05 \n",
            "Epoch: 425 | Loss: 6.548532110173255e-05 \n",
            "Epoch: 426 | Loss: 6.454645335907117e-05 \n",
            "Epoch: 427 | Loss: 6.361923442455009e-05 \n",
            "Epoch: 428 | Loss: 6.270126323215663e-05 \n",
            "Epoch: 429 | Loss: 6.180275522638112e-05 \n",
            "Epoch: 430 | Loss: 6.091277828090824e-05 \n",
            "Epoch: 431 | Loss: 6.0040958487661555e-05 \n",
            "Epoch: 432 | Loss: 5.917878297623247e-05 \n",
            "Epoch: 433 | Loss: 5.832751048728824e-05 \n",
            "Epoch: 434 | Loss: 5.74896766920574e-05 \n",
            "Epoch: 435 | Loss: 5.666119250236079e-05 \n",
            "Epoch: 436 | Loss: 5.5846750910859555e-05 \n",
            "Epoch: 437 | Loss: 5.5045322369551286e-05 \n",
            "Epoch: 438 | Loss: 5.425568815553561e-05 \n",
            "Epoch: 439 | Loss: 5.347197293303907e-05 \n",
            "Epoch: 440 | Loss: 5.270429392112419e-05 \n",
            "Epoch: 441 | Loss: 5.1948030886705965e-05 \n",
            "Epoch: 442 | Loss: 5.120077184983529e-05 \n",
            "Epoch: 443 | Loss: 5.0468013796489686e-05 \n",
            "Epoch: 444 | Loss: 4.974402691004798e-05 \n",
            "Epoch: 445 | Loss: 4.9029542424250394e-05 \n",
            "Epoch: 446 | Loss: 4.83230542158708e-05 \n",
            "Epoch: 447 | Loss: 4.76265158795286e-05 \n",
            "Epoch: 448 | Loss: 4.694239760283381e-05 \n",
            "Epoch: 449 | Loss: 4.6269164158729836e-05 \n",
            "Epoch: 450 | Loss: 4.5604912884300575e-05 \n",
            "Epoch: 451 | Loss: 4.495325993048027e-05 \n",
            "Epoch: 452 | Loss: 4.4305914343567565e-05 \n",
            "Epoch: 453 | Loss: 4.3665953853633255e-05 \n",
            "Epoch: 454 | Loss: 4.304074900574051e-05 \n",
            "Epoch: 455 | Loss: 4.242024806444533e-05 \n",
            "Epoch: 456 | Loss: 4.1809893446043134e-05 \n",
            "Epoch: 457 | Loss: 4.1210860217688605e-05 \n",
            "Epoch: 458 | Loss: 4.0618746425025165e-05 \n",
            "Epoch: 459 | Loss: 4.003552021458745e-05 \n",
            "Epoch: 460 | Loss: 3.945997741539031e-05 \n",
            "Epoch: 461 | Loss: 3.88936823583208e-05 \n",
            "Epoch: 462 | Loss: 3.833345908788033e-05 \n",
            "Epoch: 463 | Loss: 3.7782479921588674e-05 \n",
            "Epoch: 464 | Loss: 3.7238150980556384e-05 \n",
            "Epoch: 465 | Loss: 3.670358273666352e-05 \n",
            "Epoch: 466 | Loss: 3.617602851591073e-05 \n",
            "Epoch: 467 | Loss: 3.565680526662618e-05 \n",
            "Epoch: 468 | Loss: 3.5146505979355425e-05 \n",
            "Epoch: 469 | Loss: 3.4639026125660166e-05 \n",
            "Epoch: 470 | Loss: 3.4142030926886946e-05 \n",
            "Epoch: 471 | Loss: 3.364998701727018e-05 \n",
            "Epoch: 472 | Loss: 3.3169708331115544e-05 \n",
            "Epoch: 473 | Loss: 3.268940054113045e-05 \n",
            "Epoch: 474 | Loss: 3.221951919840649e-05 \n",
            "Epoch: 475 | Loss: 3.175745951011777e-05 \n",
            "Epoch: 476 | Loss: 3.129955439362675e-05 \n",
            "Epoch: 477 | Loss: 3.085126809310168e-05 \n",
            "Epoch: 478 | Loss: 3.040798219444696e-05 \n",
            "Epoch: 479 | Loss: 2.9970769901410677e-05 \n",
            "Epoch: 480 | Loss: 2.9538306989707053e-05 \n",
            "Epoch: 481 | Loss: 2.9115723009454086e-05 \n",
            "Epoch: 482 | Loss: 2.8695882065221667e-05 \n",
            "Epoch: 483 | Loss: 2.828511242114473e-05 \n",
            "Epoch: 484 | Loss: 2.7877154934685677e-05 \n",
            "Epoch: 485 | Loss: 2.747612052189652e-05 \n",
            "Epoch: 486 | Loss: 2.7083435270469636e-05 \n",
            "Epoch: 487 | Loss: 2.6692532628658228e-05 \n",
            "Epoch: 488 | Loss: 2.6309240638511255e-05 \n",
            "Epoch: 489 | Loss: 2.5930792617145926e-05 \n",
            "Epoch: 490 | Loss: 2.555905120971147e-05 \n",
            "Epoch: 491 | Loss: 2.5190582164213993e-05 \n",
            "Epoch: 492 | Loss: 2.482869786035735e-05 \n",
            "Epoch: 493 | Loss: 2.4473745725117624e-05 \n",
            "Epoch: 494 | Loss: 2.4122347895172425e-05 \n",
            "Epoch: 495 | Loss: 2.3775053705321625e-05 \n",
            "Epoch: 496 | Loss: 2.3430842702509835e-05 \n",
            "Epoch: 497 | Loss: 2.3094582502380945e-05 \n",
            "Epoch: 498 | Loss: 2.276228042319417e-05 \n",
            "Epoch: 499 | Loss: 2.243610651930794e-05 \n",
            "Prediction (after training) 4 8.00544548034668\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}